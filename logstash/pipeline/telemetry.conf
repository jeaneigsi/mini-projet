# =============================================
# Logstash Pipeline - Telemetry Processing
# =============================================
# Input: Kafka topic 'telemetry'
# Output: Elasticsearch index 'telemetry-YYYY.MM.dd'
# =============================================

input {
  kafka {
    bootstrap_servers => "kafka:29092"
    topics => ["telemetry"]
    group_id => "logstash-telemetry"
    codec => json
    decorate_events => true
  }
}

filter {
  # Parse the @timestamp field if it exists
  if [@timestamp] {
    date {
      match => ["@timestamp", "ISO8601"]
      target => "@timestamp"
    }
  }

  # Add processing metadata
  mutate {
    add_field => {
      "processed_at" => "%{+YYYY-MM-dd HH:mm:ss}"
      "pipeline" => "telemetry"
    }
  }

  # Flatten metrics for easier querying in Kibana
  if [metrics] {
    ruby {
      code => '
        metrics = event.get("metrics")
        if metrics.is_a?(Hash)
          metrics.each do |key, value|
            event.set("metric_#{key}", value)
          end
        end
      '
    }
  }

  # Ensure required fields exist
  if ![site_code] {
    mutate {
      add_field => { "site_code" => "UNKNOWN" }
    }
  }

  if ![asset_type] {
    mutate {
      add_field => { "asset_type" => "UNKNOWN" }
    }
  }

  if ![asset_code] {
    mutate {
      add_field => { "asset_code" => "UNKNOWN" }
    }
  }

  # Remove Kafka metadata fields we don't need
  mutate {
    remove_field => ["@version", "kafka"]
  }
}

output {
  # Primary output: Elasticsearch
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "telemetry-%{+YYYY.MM.dd}"
    document_type => "_doc"
  }

  # Debug output (comment out in production)
  # stdout {
  #   codec => rubydebug
  # }
}
